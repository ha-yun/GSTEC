#2주차
##<1>
* 박준영 강사님  
1. Quick,Draw  
2. Teachable machine
<hr>

##<2>
* 간단한 토너먼트 코딩
<pre><code>
import random
t=[1,2,3,4,5,6,7,8,9]
a=[]
while len(t)>0:
  one=random.choice(t)
  t.remove(one)
  if len(t)==0:
    a.append(one)
    break
  two=random.choice(t)
  t.remove(two)
  a.append((one,two))
print(a)
</code></pre>

<code><pre>
teams=list(range(1,10))
random.shuffle(teams)
half=len(teams)//2
print(teams[:half], teams[half:])
print(teams)
print(list(zip(teams[:half],teams[half:])))
</code></pre>
* html 배포 : https://app.netlify.com/teams/yun-aha/overview  
  or 깃헙 연동  
  web을 배포하면 default로 찾는 폴더 = index.html
  
<hr>

##<3>
### 머신러닝(Machine Learning) 종류
<b>
1. 지도학습 (Supervised Learning)<br>
2. 비지도학습 (Unsupervised Learning)<br>
3. 강화학습 (Reinforcement Learning)
</b>


1. 지도학습 (Supervised Learning)
- 데이터에 대한 Label(명시적인 답)이 주어진 상태에서 컴퓨터를 학습시키는 방법.
- 분류(Classification)와 회귀(Regression)로 나뉘어진다.
> (ex. 스팸 메일 분류, 집 가격 예측, 손글씨 숫자 판별, 신용카드 의심거래 감지, 의료영상 이미지기반 종양판단)

2. 비지도학습(Unsupervised Learning)  
- 데이터에 대한 Label(명시적인 답)이 없는 상태에서 컴퓨터를 학습시키는 방법.
- 데이터의 숨겨진 특징, 구조, 패턴 파악.
- 데이터를 비슷한 특성끼리 묶는 클러스터링(Clustering)과 차원축소(Dimensionality Reduction)등이 있다.
> (ex. 블로그 글 주제구분, 고객 취향별 그룹화, 웹사이트 비정상 접근 탐지, 이미지 감색 처리, 소비자 그룹 마케팅)

{좋은 입력 데이터를 만들어내는 방법 -> 특성추출(특성공학) }

3. 강화학습(Reinforcement Learning)  
- 지도학습과 비슷하지만 완전한 답(Label)을 제공하지 않는 특징이 있다.
- 기계는 더 많은 보상을 얻을 수 있는 방향으로 행동을 학습
> (ex. 게임이나 로봇 학습)


머신러닝 vs 딥러닝

|구분| Machine Learning| Deep Learning|
|---|---|---|
|훈련 데이터 크기| 작음| 큼|
|시스템 성능| 저 사양| 고 사양|
|feature 선택| 전문가 (사람) |알고리즘|
|feature 수| 많음 |적음|
|문제 해결 접근법| 문제를 분리 -> 각각 답을 얻음 -> 결과 통합| end-to-end (결과를 바로 얻음)|
|실행 시간| 짧음 |김|
|해석력 |해석 가능| 해석 어려움|

### [scikit-learn](https://scikit-learn.org/stable/index.html)
- 파이썬에 머신러닝 프레임워크 라이브러리
- 회귀, 분류, 군집, 차원축소, 특성공학, 전처리, 교차검증, 파이프라인 등 머신러닝에 필요한 기능 제공
- 학습을 위한 샘플 데이터 제공  

#### scikit-learn으로 XOR 연산 학습해보기
XOR연산?
- 두값이 서로 같으면 0, 다르면 1  (배타적 논리 합)

|P(입력)| Q(입력)| R(출력)|
|---|---|---|
| 0| 0| 0|
| 0| 1| 1|
| 1| 0| 1|
| 1| 1| 0|

<code><pre>
from sklearn import svm
* XOR의 계산 결과 데이터
xor_input=[
           # P,Q,REsult
           [0,0,0],
           [0,1,1],
           [1,0,1],
           [1,1,0]
]

1. 학습을 위해 데이터와 레이블 분리하기
xor_data=[]
xor_label=[]

for i in xor_input:
  xor_data.append([i[0],i[1]])
  xor_label.append(i[-1])
print(xor_data)
print(xor_label)

* or
for [p,q,r] in xor_input:
  xor_data.append([p,q])
  xor_label.append(r)
  
2. 데이터 학습시키기
model = svm.SVC()
model.fit(xor_data,xor_label)
   
3. 데이터 예측하기
pre=model.predict(xor_data)
print(pre)
   
4. 결과 확인하기
ok = 0
for idx,answer in enumerate(xor_label):
  p=pre[idx]
  if p==answer:ok+=1
print('정답률 : ',ok,'/',4, '=',ok/4)
   </code></pre>
   
<code><pre>
###pandas 라이브러리를 사용하여 코드 간략화
import pandas as pd
from sklearn import svm, metrics

* XOR연산
xor_input=[
           # P,Q,REsult
           [0,0,0],
           [0,1,1],
           [1,0,1],
           [1,1,0]
]

1. 입력을 학습 전용 데이터와 테스트 전용 데이터로 분류하기
xor_df = pd.DataFrame(xor_input)
xor_data = xor_df[[0,1]]
xor_label = xor_df[2]

2. 데이터 학습과 예측하기
model = svm.SVC()
model.fit(xor_data, xor_label)
pre = model.predict(xor_data)

3. 정답률 구하기
ac_score = metrics.accuracy_score(xor_label, pre)
print(ac_score)
</code></pre>

<code><pre>
### KNN 분류 모델을 이용
import pandas as pd
from sklearn import svm, metrics
from sklearn.neighbors import KNeighborsClassifier  ##분류 모델 추가

* XOR연산
xor_input=[
           # P,Q,REsult
           [0,0,0],
           [0,1,1],
           [1,0,1],
           [1,1,0]
]

1. 입력을 학습 전용 데이터와 테스트 전용 데이터로 분류하기
xor_df = pd.DataFrame(xor_input)
xor_data = xor_df[[0,1]]
xor_label = xor_df[2]

2. 데이터 학습과 예측하기
model = KNeighborsClassifier(n_neighbors=1)
model.fit(xor_data, xor_label)
pre = model.predict(xor_data)

3. 정답률 구하기
ac_score = metrics.accuracy_score(xor_label, pre)
print(ac_score)
</code></pre>
   
진행순서

>1. clf = 머신러닝모델 생성  # svm.SVC() or KNeighborsClassifier(n_neighbors=1)
>2. clf.fit(문제 , 답)
>3. 예측결과 = clf.predict(값을 얻고 싶은 데이터 )
>4. ac_score = metrics.accuracy_score(실제답, 예측결과)

clf (classifier) - scikit-learn 에서 [Estimator](https://en.wikipedia.org/wiki/Estimator) 인스턴스인 분류기를 지칭  

[머신러닝 용어집](https://developers.google.com/machine-learning/glossary)

###모델 저장과 불러오기
1. pickle --> load와 dump
<code><pre>
import pickle  
with open('xor_model.pkl','wb') as f:
  pickle.dump(model,f)  
import pickle
with open('xor_model.pkl','rb') as f:
  model = pickle.load(f)
pre = model.predict([[1,1],[1,0]])
print(pre)
pre[0],pre[1]
   </code></pre>
2. joblib -->load와 dump   
<code><pre>
from sklearn.externals import joblib
joblib.dump(model,'xor_model_2.pkl')
model = joblib.load('xor_model_2.pkl')
</code></pre>
   

#### scikit-learn 연습 01
AND 연산 모델 작성  
AND연산?
- 두값이 서로 참이면 1, 아니면 0 

|P(입력)| Q(입력)| R(출력)|
|---|---|---|
| 0| 0| 0|
| 0| 1| 0|
| 1| 0| 0|
| 1| 1| 1|

<code><pre>
### KNN 분류 모델을 이용
import pandas as pd
from sklearn import svm, metrics
from sklearn.neighbors import KNeighborsClassifier  ##분류 모델 추가

* and연산
and_input=[
           # P,Q,REsult
           [0,0,0],
           [0,1,0],
           [1,0,0],
           [1,1,1]
]

1. 입력을 학습 전용 데이터와 테스트 전용 데이터로 분류하기
and_df = pd.DataFrame(and_input)
and_data = and_df[[0,1]]
and_label = and_df[2]

2. 데이터 학습과 예측하기
model = KNeighborsClassifier(n_neighbors=1)
model.fit(and_data, and_label)
pre = model.predict(and_data)

3. 정답률 구하기
ac_score = metrics.accuracy_score(and_label, pre)
print(ac_score)
</code></pre>
   
###구글 드라이브 연동
<code><pre>
from google.colab import drive
drive.mount('/gdrive', force_remount=True)
* 구글 드라이브 파일 확인
!ls '/gdrive/My Drive/temp/'
* 반복되는 드라이브 경로 변수화
drive_path = '/gdrive/My Drive/temp/'
</code></pre>
  
<hr>

#3주차
##<1>
#### scikit-learn 연습 02

비만도 데이터 학습

- 500명의 키와 몸무게, 비만도 라벨을 이용해 비만을 판단하는 모델을 만들어보자.
<code><pre>
import pandas as pd  
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics # 평가를 위한 모듈
df = pd.read_csv(drive_path + 'bmi_500.csv', index_col='Label')
df.head()
df.info()
df.index.unique()
df.loc['Normal']
  </code></pre>
  
<code><pre>
def easy_scatter(label,color):
  t = df.loc[label]
  plt.scatter(t['Weight'],t['Height'],color=color,label=label)

plt.figure(figsize=(5, 5) )
easy_scatter('Extreme Obesity','black')
easy_scatter('Weak','blue')
easy_scatter('Normal','green')
easy_scatter('Overweight','pink')
easy_scatter('Obesity','purple')
easy_scatter('Extremely Weak','red')

plt.legend()
plt.show()
</code></pre>

###모델링
1. 문제와 답으로 분리
2. 훈련셋과 평가셋으로 분리
3. 모델생성 및 하이퍼파라미터 조정
4. 학습 및 평가

<code><pre>
data=pd.read_csv(drive_path + 'bmi_500.csv')

X=data.loc[:,'Height':'Weight']  #숫자로 가져오고 싶으면 iloc, 문자면 loc
y=data.loc[:,'Label']
print(X.shape)
print(y.shape)

X_train = X.iloc[:350, :]
X_test = X.iloc[350:, :]
y_train = y.iloc[:350]
y_test = y.iloc[350:]

bmi_model = KNeighborsClassifier(n_neighbors=10)
bmi_model.fit(X_train, y_train)
pre=bmi_model.predict(X_test)
metrics.accuracy_score(y_test,pre)

bmi_model.predict([[185, 43], [100,20]])
</code></pre>

### 머신러닝(Machine Learning) 진행 과정
<b>
1. ProblemIdentification (문제정의) <br>
2. Data Collect(데이터 수집) <br>
3. Data Preprocessing(데이터 전처리) <br>
4. EDA(탐색적 데이터분석) <br>
5. Model 선택, Hyper Parameter 조정 <br>
6. 학습 <br>
7. 모델 Evaluation(평가)
</b>

1. 문제정의
 - 지도학습 : 분류, 회귀
 - 비지도학습 : 군집, 차원축소
 - 강화학습

2. 데이터 수집
 - File Data, Database, 공공데이터, kaggle
 - Web Crawler (뉴스, SNS, 블로그)
 - IoT 센서를 통한 수집

3. 데이터 전처리
 - 결측치, 이상치 수정
 - Encoding : Categorical Data를 수치 데이터로 변경, 원핫인코딩
 - Feature Engineering (특성공학) : 단위 변환, 새로운 속성 추가 (MinMaxScaler, StandardScaler, RobustScaler)

4. EDA
 - 시각화를 통해 특성 선택 : (scatterplot, pairplot, boxplot, heatmap)
 - 사용할 Feature 선택 : 전처리 전략수립

5. Model 선택, Hyper Parameter 조정
 - 목적에 맞는 적절한 모델 선택
  - 지도학습
	 - 분류 : knn, Logistic Regression, SVM, Decision Tree, RandomForest, GradientBoosting
	 - 회귀 : knn, Linear Regression, Lasso, Ridge, Decision Tree, RandomForest, GradientBoosting
 - 하이퍼파라미터 튜닝

6. 학습
 - model.fit(X_train, y_train) : train 데이터와 test 데이터를 7:3 정도로 나눔 (train_test_split)
 - model.predict(X_test) :  (cross_val_score)

7. 평가
 -	지도학습
	 - 분류 : 정확도, 정밀도, 재현율, f1-score
	 - 회귀 : R^2, MSE, RMSE

 -	비지도학습
	- ARI 값
      
###moral machine ,, codeorg

<hr>

##<2> 
##데이터 수집
### 수집 데이터 형태
* 정형 – 일정한 규격에 맞춰서 구성된 데이터 (어떠한 역할을 알고 있는 데이터)
    - 관계형 데이터베이스 시스템의 테이블과 같이 고정된 컬럼에 저장되는 데이터 파일 등이 될 수 있다.
     즉, 구조화 된 데이터가 정형 데이터

* 반정형 – 일정한 규격으로 구성되어 있지 않지만 일정한 틀을 갖추기 위해서 태그나 인덱스형태로 구성된 데이터
    - 연산이 불가능한 데이터 ex) XML. HTML, JSON 등

* 비정형 – 구조화 되지 않는 형태의 데이터 (정형과 반대로 어떠한 역할인지 알수 없는 데이터)
    - 형태가 없으며, 연산도 불가능한 데이터 ex) SNS, 영상, 이미지, 음성, 텍스트 등
    >우리가 주로 수집할 데이터들은 반정형 혹은 비정형 데이터라고 보면 된다.

### 스크레이핑, 크롤링
- Scraping: 웹 사이트의 특정 정보를 추출하는 것. 웹 데이터의 구조 분석이 필요
- 로그인이 필요한 경우가 많다
- Crawling: 프로그램이 웹사이트를 정기적으로 돌며 정보를 추출하는 것 (이러한 프로그램을 크롤러, 스파이더라고 한다)

### 웹 크롤러 (Web Crawler)
* 인터넷에 있는 웹 페이지로 이동해서 데이터를 수집하는 프로그램
* 크롤러 = 스크래퍼, 봇, 지능 에이전트, 스파이더 등으로 불림

### 웹 크롤링을 위해 알아둘 것
* Web 구조
    - 클라이언트 = 서버에 정보 또는 서비스를 요청하는 컴퓨터/소프트웨어/사용자 ex) 웹브라우저
    - 서버 = 정보를 보관하고 클라이언트가 요청한 정보 서비스를 제공해주는 컴퓨터/소프트웨어 ex) 영상, 파일, 채팅, 게임, 웹서버

* URL 구조
    - http://192.168.0.10:9000/WebProject?msg=Hello
    - 웹 문서를 교환하기 위한 규칙
    - 주소 또는 IP
    - 포트번호
    - 리소스 경로
    - 쿼리스트링

* 데이터 전송 방식
    - GET, POST  (대표적인 2가지)

* 패킷(Packet) 형식
    - 요청패킷: 클라이언트에서 필요한 헤더 Key/Value를 세팅한 후 요청, 전달
    -응답패킷: 서버에 필요한 Key/Value를 세팅한 후, 응답, 전달
      
* URI는 인터넷에 있는 자원을 나타내는 유일한 주소다. Uniform Resoure Identifier  
  (하위개념으로 url, urn이 있다.)
* url  :  웹  기본 80포트  
* w3school.com : W3스쿨즈는 온라인으로 웹 기술을 배우는 교육용 웹 사이트이다


### 웹 페이지 구성 3요소
* HTML, CSS, Javascript 
  * HTML: Tag, Element, Attribute, Content
  * CSS : 선택자 { 스타일 속성 : 스타일 값; }
    - 선택자(셀렉터) : tag, id, class
  * Javascript :Web Page에서 어떤 동작에 대한 반응이 일어날 수 있도록 해주는 언어
     - DOM (Document Object Model) : 문서를 객체로 조작하기 위한 표준 모델. HTML 문서를 객체로 표현할 때 사용하는 API
    

* 데이터 추출 방법
    -  웹 페이지를 방문하고 크롤링하려는 단어나 문구를 검색하여 찾아낸 정보를 수집하여 기록
    - 이때 파일로 기록해서 저장하거나 데이터베이스에 저장 (저장하는 과정에서 단어나 문구와 연관된 결과 값에 인덱스를 부여)

* 웹 크롤링을 위한 라이브러리   
    -requests: 접근할 웹 페이지의 데이터를 요청/응답받기 위한 라이브러리
    - BeautifulSoup : 응답받은 데이터 중 원하는 데이터를 추출하기 위한 라이브러리

### urllib 사용법
- url 관련 데이터를 처리하는 라이브러리
- http 또는 ftp를 사용해 데이터를 다운로드 받는데 사용

#### 웹에서 파일 다운로드하기
<code><pre>
import requests as req
res = req.get('https://www.google.com')  #response 200번대면 잘 연결된것. 400이면 주소 오류일 수도 있다.
res.text
url='https://www.naver.com'
res=req.get(url)
res   #response[200]

* 다른프로그램에서 링크를 들어가는게 안되는 페이지
    - headers로 해결
headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',
        'referer':'https://nid.naver.com/login/sso/finalize.nhn?url=https%3A%2F%2Fwww.naver.com&sid=KBwWbyEJdJL521l3&svctype=1'}

url = 'https://prod.danawa.com/list/?cate=11229515&logger_kw=ca_main_more'
res = req.get(url, headers=headers)
res.text

* json 읽는법
import json
from pandas.io.json import json_normalize
url = 'http://rank.search.naver.com/rank.js'
res = req.get(url)
json_normalize(json.loads(res.text), ['data', 'data'])


* 이미지 가져오기
import urllib.request
url = "https://upload.wikimedia.org/wikipedia/commons/4/47/PNG_transparency_demonstration_1.png"
urllib.request.urlretrieve(url, 'test.png')
</code></pre>
  
#### urlopen() 사용법
- 위의 urlretrive()는 데이터를 파일에 바로 저장하였다.
- urlopen()을 사용하면 데이터를 파이선에서 읽을 수 있다.
<code><pre>
png = urllib.request.urlopen(url).read()  
with open('test2.png','wb')as f:
  f.write(png)
  </code></pre>
  
#### 웹 API 이용하기
- 클라이언트정보를 보여주는 샘플 api 사이트 접속
<code><pre>
url = "http://api.aoikujira.com/ip/ini"  
res = urllib.request.urlretrieve(url)
  </code></pre>
  
####  GET  요청을 사용하여 파라미터를 보내는 경우
- URL 끝 부분에 ?를 입력하고 key = value 형식으로 매개변수를 추가한다. 여러개의 파라미터를 넣는 경우 &를 사용하여 구분한다
- 한글 등이 파라미터로 사용될 때는 반드시 이러한 코딩을 해주어야 한다

<code><pre>
url = "http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp"
res = urllib.request.urlopen(url)
data = res.read()
text = data.decode('utf-8')
text
</code></pre>

###스크레이핑 : 웹에서 원하는 정보를 추출하는 것  
HTML과 XML 문서에서 정보를 추출할 수 있다

### BeautifulSoup 라이브러리
HTML 문자열을 파이썬에서 사용 가능한 객체로 변환
BeautifulSoup( markup, parser )
* markup : requests로 요청/응답 받은 변수
* parser : 원시 코드인 순수 문자열 객체를 해석할 수 있도록 분석  
 - 문자열을 파이썬에서 사용할 수 있도록 해석해주는 프로그램 (lxml, html.parser, html5lib)

데이터를 추출하기 위한 속성

|속성|설명|
|-|-|
|text|하위 태그에 대한 값 전부 출력|
|string|정확히 태그에 대한 값만 출력|

원하는 요소 접근하는 메소드

|메소드|설명|
|-|-|
|find(tag),
 find(tag, id=값),
 find(tag, class=값), 
 find(tag, attr{속성:속성값})|원하는 태그를 하나만 반환|
|find_all(),
 find_all(tag, limit=숫자)|원하는 태그를 리스트 형태로 반환|
|select(CSS Selector)|CSS Selector를 활용하여 원하는 태그를 리스트 형태로 반환|
|extract()|태그를 지우는 기능|


##### !pip3 install beautifulsoup4 -> beautifulsoup설치하는 방법
![img.png](img.png)
#### id 를 사용하는 방법
- 위와 같이 내부 구조를 일일이 파악하고 코딩하는 것은 복잡하다
- find()를 사용하여 간단히 원하는 항목을 찾을 수 있다

![img_1.png](img_1.png)
![img_2.png](img_2.png)

### DOM 요소 파악하기
- Document Object Model: XML이나 HTML 요소에 접근하는 구조를 나타낸다
- DOM 요소의 속성이란 태그 뒤에 나오는 속성을 말한다 < a > 태그의 속성은 href이다
![img_3.png](img_3.png)
  
### urlopen() 사용 하기
![img_4.png](img_4.png)

### CSS 선택자 사용하기
- CSS 선택자를 사용해서 원하는 요소를 추출할 수 있다.
- h1 과 li 태그를 추출하는 코드
![img_5.png](img_5.png)
  
### CSS 자세히 알아보기
- 웹 페이지의 검사 메뉴를 선택 (우측 버튼)
- 특정 태그를 선택하고 다시 우측 버튼을 누르고 Copy - Copy selector를 선택하면 CSS 선택자가 클립보드에 저장된다 (아래 예시)

#mw-content-text > div > ul:nth-child(6) > li > b > a

- 위에서 nth-child(6)은 6번째에 있는 요소를 가리킨다
- 이를 기반으로 작품목록을 가져오는 프로그램을 작성하겠다.

![img_6.png](img_6.png)

<hr>
